---
title: "AirBNB L.A. Price Prediction"
subtitle: "UCSB PSTAT 131 Final Project"
author: "Brian Che"
date: "Fall 2022"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](images/AirBNB_logo.png){width="25%"} ![](images/AirBNB_LA.jpg){width="44%"}

## Introduction

This machine learning project consists of building 4 different predictive models to find the best predictor of AirBnB Los Angeles listing prices through regression.

### What is AirBnB?

AirBnB, standing for "Air Bed and Breakfest" is a vacational rental company that serves as an online marketplace that connects people who want to rent out their homes with people who are looking for accomodations at their preferred destinations. Travelers are able to rent a space for multiple people to share, a shared space with private rooms, or the entire property for themselves for a specific duration of days. Essentially, each listing is set at minimum price per night by the owner for their property that is caclulated into the toal pricing of their stay, which may usually include service fees, tax, etc.

### Overview of the Dataset

The dataset I'll be using is from "[Inside AirBnB](http://insideairbnb.com/get-the-data/)" which is a mission driven project that provides data and advocacy about Airbnb's impact on residential communities, and is specifically focused on the Los Angeles, California, U.S. region at a compile date of September 9, 2022. The dataset consists of 45,815 rows (listings) and 18 predictors that serve as the listing background information.

### Our Focus

With new hosts wanting to add their property for rent on the platform, a competitive and reasonable price charged per night must be established for success in renting and appealing to consumers. While this will obviously vary through factors such as market value price, the neighborhood area, or simply being within an attraction or tourist hot spot, we can use machine learning to simplify this process of predicting housing prices to utilize, using this vast dataset with thousands of previously established listings.

### Load libraries and data

We first load the libraries we will be using and reading in the data, taking a look at the 18 variables.

```{r,warning=FALSE,message=FALSE, class.source = 'fold-hide'}
# load libraries
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(yardstick)
library(corrplot)
library(randomForest)
library(xgboost)
library(vip)
library(rpart.plot)
library(ranger)
library(kernlab)
library(kknn)
library(baguette)

# read in data (45)
airbnb <- read_csv("data/unprocessed/listings.csv")
```

## Exploratory Data Analysis

By performing EDA, we can get a better understanding of the data and ultimately help us determine the predictors that we will be including within our recipe for our response variable `price`. Specifically, I will be taking a look at the relationships price may have with the other variables. Throughout this process, I will also be cleaning the data to prevent errors that may be occurring as we run our code analysis.

### Data Cleaning

```{r}
airbnb %>% glimpse()
```

From a general outlook, we can see that there are 11 numeric variables and 7 non-numeric variables of data type `chr`, which will help us condense our dataset into the variables we'll be prioritizing from ones that may be more irrelevant.

```{r, class.source = 'fold-hide'}
sapply(airbnb, function(x) sum(is.na(x)))
```

The variables with missing values that stick out to us are `neighbourhood_group` and `reviews_per_month`, which we'll need to manipulate the data to resolve. `neighbourhood_group`== is an important

```{r}
airbnb$reviews_per_month[is.na(airbnb$reviews_per_month)] <- 0

airbnb$neighbourhood_group <- factor(airbnb$neighbourhood_group, levels = c('City of Los Angeles', 'Other Cities', 'Unincorporated Areas'))

airbnb$room_type <- factor(airbnb$room_type, levels = c('Entire home/apt', 'Hotel room', 'Private room', 'Shared room'))

airbnb <- airbnb %>%
  rename(host_listings_count = calculated_host_listings_count) %>%
  # remove the 14 rows with price equal to 0 because a listing price of 0 can't be possible
  filter(price != 0) %>%
  filter(availability_365 != 0) %>%
  filter(!is.na(neighbourhood_group)) %>%
  select(-id, -name, -host_id, -host_name, -last_review, -license)

set.seed(2022)
airbnb <- airbnb[sample(nrow(airbnb), size=15000), ]
```

```{r, class.source = 'fold-hide'}
airbnb %>%
  summary()
```

```{r, class.source = 'fold-hide'}
# include use="pairwise.complete.obs" to exclude NAs
corrplot(cor(Filter(is.numeric, airbnb)), method = 'color', )
```

```{r}
# drop number_of_reviews_ltm and reviews_per_month for being highly correlated with number_of_reviews
airbnb_clean <- airbnb %>%
  select(-neighbourhood, -number_of_reviews_ltm, -reviews_per_month)
```

```{r,  warning=FALSE, message=FALSE, class.source = 'fold-hide'}
write_rds(airbnb_clean, "data/processed/airbnb_clean.rds")
```

```{r, class.source = 'fold-hide'}
# use corrplot to make correlation of cont vars
corrplot(cor(Filter(is.numeric, airbnb_clean)), method = 'color')
```

For sake of faster runtime, we wil

```{r, class.source = 'fold-hide'}
NAnalysis <- airbnb_clean %>% 
  group_by(neighbourhood_group) %>% summarise(Mean_Price = mean(price))

ggplot(NAnalysis, aes(x = reorder(neighbourhood_group, -Mean_Price), y = Mean_Price, fill=neighbourhood_group)) + 
  geom_bar(stat="identity", show.legend = FALSE) + 
  labs(title="Average Price of Rooms in each Neighbourhood Group") + 
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5), legend.position = c(0.8, 0.5)) + xlab("") + ylab("Mean Price")
```

```{r, class.source = 'fold-hide'}
ggplot(airbnb_clean, aes(x = room_type, y = price)) +
  geom_boxplot(aes(fill = room_type)) + scale_y_log10() +
  xlab("Room Type") + 
  ylab("Price") +
  ggtitle("Housing Price by Room Type") +
  geom_hline(yintercept = mean(airbnb$price), color = "purple", linetype = 2)
```

## Data Splitting & Cross-validation

-   Describe your process of splitting data into training, test, and/or validation sets.

```{r}
set.seed(2022)
airbnb_split <- airbnb_clean %>% 
  mutate(price = log(price)) %>%
  initial_split(strata = price, prop = 0.8) 

airbnb_train <- training(airbnb_split)
airbnb_test <- testing(airbnb_split)

dim(airbnb_train)
dim(airbnb_test)
```

```{r}
airbnb_train %>%
  summary()
```

-   Describe the process of cross-validation.

### Cross Validation Folding on Training

```{r, warning=FALSE,message=FALSE}
set.seed(2022)
airbnb_fold <- vfold_cv(airbnb_train, v = 10, strata = price)
```

## Model Fitting

-   Describe the types of models you fit, their parameter values, and the results.

Tune for: Boosting, Random Forest Model, K Nearest Neighbors Model, Bagging

### Recipe Building

airbnb_train

```{r, warning=FALSE,message=FALSE}
airbnb_recipe <- recipe(price ~ . , data = airbnb_train) %>%
  step_dummy(all_nominal_predictors())
```

### xgboost model

```{r, warning=FALSE, message=FALSE, eval=FALSE}

set.seed(2022)

# trees = tune(), tree_depth = 8
boost_spec <- boost_tree(trees = tune(), 
                         tree_depth = 8) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_grid = grid_regular(trees(range = c(10, 500)), levels = 10)

boost_wf = workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(airbnb_recipe)

boost_tune_res <- tune_grid(boost_wf,
                            resamples = airbnb_fold,
                            grid = boost_grid)
```


```{r, class.source = 'fold-hide'}
# save(boost_tune_res, boost_wf, boost_grid, boost_spec, file = "scripts/model_fitting/boost_model.rda")

load(file='scripts/model_fitting/boost_model.rda')
```

```{r}
autoplot(boost_tune_res)
```

```{r}
show_best(boost_tune_res, metric = "rsq")
show_best(boost_tune_res, metric = "rmse")
```

```{r, warning=FALSE,message=FALSE}
best_trees <- select_best(boost_tune_res)

boost_final <- finalize_workflow(boost_wf, best_trees)

boost_fit <- boost_final %>% 
  fit(airbnb_train)
```

### Random Forest Model

```{r, warning=FALSE,message=FALSE, eval=FALSE}
set.seed(2022)
rand_forest_spec <- rand_forest(mtry = tune(), 
                                trees = tune(), 
                                min_n = tune()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("regression")

rand_forest_wf <- workflow() %>%
  add_model(rand_forest_spec) %>%
  add_recipe(airbnb_recipe)

rand_forest_grid <- grid_regular(mtry(range = c(1,8)),
                                      trees(range = c(1, 8)),
                                 min_n(range = c(1, 8)),  
                                       levels = 4)

rf_tune <- rand_forest_wf %>% 
  tune_grid(
    resamples = airbnb_fold, 
    grid = rand_forest_grid)
```

```{r, class.source = 'fold-hide', class.source = 'fold-hide'}
# save(rf_tune, rand_forest_wf, rand_forest_grid, rand_forest_spec, file = "scripts/model_fitting/rf_model.rda")
load('scripts/model_fitting/rf_model.rda')
```

```{r}
autoplot(rf_tune)
```



```{r}
show_best(rf_tune, metric = "rsq")

show_best(rf_tune, metric = "rmse")
```

```{r, warning=FALSE,message=FALSE}
best_mtry <- select_best(rf_tune)

rf_final <- finalize_workflow(rand_forest_wf, best_mtry)

rf_fit <- rf_final %>% 
  fit(airbnb_train)
```

### K Nearest Neighbours Model

```{r, warning=FALSE,message=FALSE, eval=FALSE}
set.seed(2022)

knn_spec <- nearest_neighbor(
    neighbors = tune(),
    mode = "regression") %>% 
  set_engine("kknn")

knn_wf <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(airbnb_recipe)

# set-up tuning grid ----
knn_params <- parameters(knn_spec)

# define grid
knn_grid <- grid_regular(knn_params, levels = 8)

knn_tune <- knn_wf %>% 
  tune_grid(
    # what will it fit the workflow to
    resamples = airbnb_fold, 
    # how does it complete the models in those workflows
    grid = knn_grid)
```

```{r warning=FALSE,message=FALSE, class.source = 'fold-hide'}
# save(knn_tune, knn_wf, knn_grid, knn_spec, file = "scripts/model_fitting/knn_model.rda")
load('scripts/model_fitting/knn_model.rda')
```

```{r}
autoplot(knn_tune)
```

```{r}
show_best(knn_tune, metric = "rsq")

show_best(knn_tune, metric = "rmse")
```

```{r, warning=FALSE,message=FALSE}
best_knn <- select_best(knn_tune)

knn_final <- finalize_workflow(knn_wf, best_knn)

knn_fit <- knn_final %>% 
  fit(airbnb_train)
```

### Bagging

```{r, warning=FALSE,message=FALSE, eval=FALSE}
set.seed(2022)

bag_spec <-
  bag_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  set_engine("rpart", times = 10) %>%
  set_mode("regression")

bag_wf <-
  workflow() %>%
  add_recipe(airbnb_recipe) %>%
  add_model(bag_spec)

bag_params <- parameters(bag_spec)

bag_grid <- grid_regular(bag_params, levels = 4)

bag_tune <- bag_wf %>% 
  tune_grid(
    # what will it fit the workflow to
    resamples = airbnb_fold, 
    # how does it complete the models in those workflows
    grid = bag_grid)
```

```{r, warning=FALSE,message=FALSE, class.source = 'fold-hide'}
# save(bag_tune, bag_wf, bag_grid, bag_spec, file = "scripts/model_fitting/bagging_model.rda")
load('scripts/model_fitting/bagging_model.rda')
```

```{r}
autoplot(bag_tune)
```

```{r}
show_best(bag_tune, metric = "rsq")

show_best(bag_tune, metric = "rmse")
```

```{r, warning=FALSE,message=FALSE}
best_cost <- select_best(bag_tune)

bag_final <- finalize_workflow(bag_wf, best_cost)

bag_fit <- bag_final %>% 
  fit(airbnb_train)
```

## Model Selection & Performance

-   A table and/or graph describing the performance of your best-fitting model on testing data.
-   Describe your best-fitting model however you choose, and the quality of its predictions, etc.

```{r, warning=FALSE,message=FALSE, class.source = 'fold-hide'}
boost_rsq <- augment(boost_fit, new_data = airbnb_test)%>%
  rsq(truth = price, estimate = .pred) 

boost_rmse <- augment(boost_fit, new_data = airbnb_test)%>%
  rmse(truth = price, estimate = .pred) 

rf_rsq <- augment(rf_fit, new_data = airbnb_test)%>%
  rsq(truth = price, estimate = .pred)

rf_rmse <- augment(rf_fit, new_data = airbnb_test)%>%
  rmse(truth = price, estimate = .pred)

knn_rsq <- augment(knn_fit, new_data = airbnb_test)%>%
  rsq(truth = price, estimate = .pred)

knn_rmse <- augment(knn_fit, new_data = airbnb_test)%>%
  rmse(truth = price, estimate = .pred)

bag_rsq <- augment(bag_fit, new_data = airbnb_test)%>%
  rsq(truth = price, estimate = .pred)

bag_rmse <- augment(bag_fit, new_data = airbnb_test)%>%
  rmse(truth = price, estimate = .pred)

model_names <- c("Boosting", "Random Forest", "K-Nearest Neighbors", "Bagging")

#
all_model_rsq <- c(boost_rsq$.estimate, rf_rsq$.estimate, knn_rsq$.estimate, bag_rsq$.estimate)
 
# 
all_model_rmse <- c(boost_rmse$.estimate, rf_rmse$.estimate, knn_rmse$.estimate, bag_rmse$.estimate)

all_model_results <- tibble(Model = model_names,
                             Rsq = all_model_rsq,
                            RMSE = all_model_rmse)
```

```{r}
all_model_results %>%
  arrange(desc(Rsq))
```

```{r, class.source = 'fold-hide'}
augment(boost_fit, new_data = airbnb_test) %>%
  ggplot(aes(price, .pred)) +
  geom_abline() +
  geom_point(alpha = 0.5)
```

## Conclusion

-   Discusses the outcome(s) of models you fit.
-   Which models performed well, which performed poorly?
-   Were you surprised by model performance?
-   Next steps?
-   General conclusions?

If I were to do this again, I would include the "names" variable, which is the title the hosts manually input for their listings, to tokenize and see if certain words or phrases may correlate to the prices (e.g. "luxury" or "mansion" which may have more expensive values).
