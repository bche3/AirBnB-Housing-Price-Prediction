---
title: "AirBNB L.A. Price Prediction"
subtitle: "UCSB PSTAT 131 Final Project"
author: "Brian Che"
date: "Fall 2022"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](images/AirBNB_logo.png){width="25%"} ![](images/AirBNB_LA.jpg){width="44%"}

## Introduction

This machine learning project consists of building 4 different predictive models to find the best predictor of AirBnB Los Angeles listing prices through regression.

### What is AirBnB?

AirBnB, standing for "Air Bed and Breakfest" is a vacational rental company that serves as an online marketplace that connects people who want to rent out their homes with people who are looking for accomodations at their preferred destinations. Travelers are able to rent a space for multiple people to share, a shared space with private rooms, or the entire property for themselves for a specific duration of days. Essentially, each listing is set at minimum price per night by the owner for their property that is caclulated into the toal pricing of their stay, which may usually include service fees, tax, etc.

### Overview of the Dataset
The dataset I'll be using is from "[Inside AirBnB](http://insideairbnb.com/get-the-data/)" which is a mission driven project that provides data and advocacy about Airbnb's impact on residential communities, and is specifically focused on the Los Angeles, California, U.S. region at a compile date of September 9, 2022.


- Describes the data, the research questions, provides any background readers need to understand your project, etc.


### Load libraries and data
```{r,warning=FALSE,message=FALSE, class.source = 'fold-hide'}
# load libraries
library(tidyverse)
library(tidymodels)
library(yardstick)
library(corrplot)
library(randomForest)
library(xgboost)
library(vip)
library(rpart.plot)
library(ranger)
library(kernlab)
library(kknn)

# read in data (45)
airbnb <- read_csv("data/unprocessed/listings.csv")
```


## Exploratory Data Analysis

- at least 3 to 5 visualizations and/or tables and their interpretation/discussion
- create a univariate visualization of the outcome(s)
- create a bi-variate or multivariate visualization of the relationship(s) between the outcome and select predictors, etc
- Part of an EDA involves asking questions about your data and exploring your data to find the answers.

<https://www.kaggle.com/code/upadorprofzs/understand-your-data-airbnb-reservations>

### Data Cleaning
```{r}
airbnb %>% glimpse()
```

```{r}
sapply(airbnb, function(x) sum(is.na(x)))

airbnb$reviews_per_month[is.na(airbnb$reviews_per_month)] <- 0

airbnb$neighbourhood_group <- factor(airbnb$neighbourhood_group, levels = c('City of Los Angeles', 'Other Cities', 'Unincorporated Areas'))

airbnb$room_type <- factor(airbnb$room_type, levels = c('Entire home/apt', 'Hotel room', 'Private room', 'Shared room'))

airbnb <- airbnb %>%
  rename(host_listings_count = calculated_host_listings_count) %>%
  # remove the 14 rows with price equal to 0 because a listing price of 0 can't be possible
  filter(price != 0) %>%
  filter(availability_365 != 0) %>%
  filter(!is.na(neighbourhood_group)) %>%
  select(-id, -name, -host_id, -host_name, -last_review, -license) %>%
  glimpse()

set.seed(2022)
airbnb <- airbnb[sample(nrow(airbnb), size=15000), ]
```



```{r}
airbnb %>%
  summary()

# include use="pairwise.complete.obs" to exclude NAs
corrplot(cor(Filter(is.numeric, airbnb)), method = 'color', )

# drop neighbourhood, latitude, longitude,
# drop number_of_reviews_ltm and reviews_per_month for being highly correlated with number_of_reviews
airbnb_clean <- airbnb %>%
  select(-neighbourhood, -number_of_reviews_ltm, -reviews_per_month)
```


```{r}
write_rds(airbnb_clean, "data/processed/airbnb_clean.rds")

# load("/data/processed/airbnb_clean.rds")
```

```{r}
corrplot(cor(Filter(is.numeric, airbnb_clean)), method = 'color')

```
For sake of faster runtime, we wil 

```{r}
NAnalysis <- airbnb_clean %>% 
  group_by(neighbourhood_group) %>% summarise(Mean_Price = mean(price))

ggplot(NAnalysis, aes(x = reorder(neighbourhood_group, -Mean_Price), y = Mean_Price, fill=neighbourhood_group)) + 
  geom_bar(stat="identity", show.legend = FALSE) + 
  labs(title="Average Price of Rooms in each Neighbourhood Group") + 
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5), legend.position = c(0.8, 0.5)) + xlab("") + ylab("Mean Price")
```


```{r}
# use corrplot to make correlation of cont vars
corrplot(cor(Filter(is.numeric,airbnb_clean)), method = 'color')
```

## Data Splitting & Cross-validation

- Describe your process of splitting data into training, test, and/or validation sets. 

```{r}
set.seed(2022)
airbnb_split <- airbnb_clean %>% 
  mutate(price = log(price)) %>%
  initial_split(strata = price, prop = 0.8) 

airbnb_train <- training(airbnb_split)
airbnb_test <- testing(airbnb_split)

dim(airbnb_train)
dim(airbnb_test)
```
```{r}
airbnb_train %>%
  summary()
```


- Describe the process of cross-validation.

### Cross Validation Folding on Training
```{r}
set.seed(2022)
airbnb_fold <- vfold_cv(airbnb_train, v = 10, strata = price)
```

```{r}
# save()
```

## Model Fitting

- Describe the types of models you fit, their parameter values, and the results.


Tune for: 
  Random Forest Model, K Nearest Neighbors Model, Boosted Tree, ? SVM Model

Don't tune for:
  Ridge Regression Model, Lasso Regression Model

### Recipe Building

airbnb_train

```{r}
# airbnb_recipe <- recipe(price ~ ., data = airbnb_train) %>%
#   step_dummy(all_nominal_predictors()) %>% 
#   step_normalize(all_predictors())
#   # step_center(all_predictors()) %>% 
#   # step_scale(all_predictors())

airbnb_recipe <- recipe(price ~ . , data = airbnb_train) %>%
  step_dummy(all_nominal_predictors())

airbnb_recipe
```


### xgboost model

```{r}
set.seed(2022)

# trees = tune(), tree_depth = 8
boost_spec <- boost_tree(trees = tune(), 
                         tree_depth = 8) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_grid = grid_regular(trees(range = c(10, 2000)), levels = 8)

boost_wf = workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(airbnb_recipe)

boost_tune_res <- tune_grid(boost_wf,
                            resamples = airbnb_fold,
                            grid = boost_grid)

autoplot(boost_tune_res)

boost_tune_res_metrics = collect_metrics(boost_tune_res)
arrange(boost_tune_res_metrics, desc(mean))
```
```{r}
save(boost_tune_res, boost_wf, boost_grid, boost_spec, file = "scripts/model_fitting/boost_model.rda")
```

### Random Forest Model

```{r}
set.seed(2022)
rand_forest_spec <- rand_forest(mtry = tune(), 
                                trees = tune(), 
                                min_n = tune()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("regression")



rand_forest_wf <- workflow() %>%
  add_model(rand_forest_spec) %>%
  add_recipe(airbnb_recipe)

# rand_forest_tune_res <- tune_grid(rand_forest_wf,
#                                   resamples = airbnb_fold,
#                                   grid = rand_forest_grid,
#                                   )



# rf_params <- parameters(rand_forest_spec) %>% 
#   update(mtry = mtry(range= c(2, 6)))
# 
# 
# rf_grid <- grid_regular(rf_params, levels = 2)

rand_forest_grid <- grid_regular(mtry(range = c(1,8)),
                                      trees(range = c(1, 8)),
                                 min_n(range = c(1, 8)),  
                                       levels = 4)


rf_tune <- rand_forest_wf %>% 
  tune_grid(
    resamples = airbnb_fold, 
    # how does it complete the models in those workflows
    grid = rand_forest_grid)


autoplot(rf_tune)
```

```{r}
# Save
save(rf_tune, rand_forest_wf, rand_forest_grid, rand_forest_spec, file = "scripts/model_fitting/rf_model.rda")
```

```{r}


collect_metrics(rand_forest_tune_res, metric = ) %>%   
  arrange(mean)

show_best(rand_forest_tune_res, metric = "rsq")

show_best(rand_forest_tune_res, metric = "rmse")

final_rf <- rand_forest_wf %>%
  finalize_workflow(select_best(rf_tune))

final_rf
```



### K Nearest Neighbours Model

```{r}
set.seed(2022)

knn_spec <- nearest_neighbor(
    neighbors = tune(),
    mode = "regression") %>% 
  set_engine("kknn")

knn_wf <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(airbnb_recipe)

# set-up tuning grid ----
knn_params <- parameters(knn_spec)


# define grid
knn_grid <- grid_regular(knn_params, levels = 8)

knn_tune <- knn_wf %>% 
  tune_grid(
    # what will it fit the workflow to
    resamples = airbnb_fold, 
    # how does it complete the models in those workflows
    grid = knn_grid)

autoplot(knn_tune)
```
```{r}
save(knn_tune, knn_wf, knn_grid, knn_spec, file = "scripts/model_fitting/knn_model.rda")
```

```{r}
collect_metrics(knn_tune, metric = ) %>%   
  arrange(mean)

show_best(knn_tune, metric = "rsq")

show_best(knn_tune, metric = "rmse")
```

```{r}
set.seed(2022)

bag_spec <-
  bag_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) %>%
  set_engine("rpart", times = 10) %>%
  set_mode("regression")

bag_wf <-
  workflow() %>%
  add_recipe(airbnb_recipe) %>%
  add_model(bag_spec)

# bag_grid <- grid_regular(
#   tree_depth(),
#   levels = 10
# )

bag_params <- parameters(bag_spec)

bag_grid <- grid_regular(bag_params, levels = 4)

# bag_grid <- grid_regular(mtry(range = c(1,8)),
#                                       trees(range = c(1, 8)),
#                                  min_n(range = c(1, 8)),  
#                                        levels = 4)

bag_tune <- bag_wf %>% 
  tune_grid(
    # what will it fit the workflow to
    resamples = airbnb_fold, 
    # how does it complete the models in those workflows
    grid = bag_grid)

autoplot(bag_tune)
```
```{r}
save(bag_tune, bag_wf, bag_grid, bag_spec, file = "scripts/model_fitting/bagging_model.rda")
```

```{r}
collect_metrics(bag_tune, metric = ) %>%   
  arrange(mean)

show_best(bag_tune, metric = "rsq")

show_best(bag_tune, metric = "rmse")

```


### Comparing model accuracies for training
```{r}
best_model <- select_best(tune_res, metric = "roc_auc")

en_final <- finalize_workflow(en_workflow, best_model)

en_final_fit <- fit(en_final, data = pokemon_train)

predicted_data <- augment(en_final_fit, new_data = pokemon_test) %>% 
  select(type_1, starts_with(".pred"))

```



## Model Selection & Performance

- A table and/or graph describing the performance of your best-fitting model on testing data. 
- Describe your best-fitting model however you choose, and the quality of its predictions, etc.

## Conclusion

- Discusses the outcome(s) of models you fit. 
- Which models performed well, which performed poorly? 
- Were you surprised by model performance? 
- Next steps? 
- General conclusions?


